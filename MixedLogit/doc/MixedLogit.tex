\documentclass[12pt]{article}

\usepackage[titletoc]{appendix}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{multicol}

\usepackage[margin=1in]{geometry}

\title{Mixed logit model}
\author{Ludovic Stourm}

\begin{document}
\maketitle

\section{Non-spatial model setup}
In a market $m$, there are $N_m$ decision-makers drawn from a population characterized by a heterogeneity distribution $f$. There are $J$ alternatives (indexed by $j$), each one is defined by a vector of characteristics $\textbf{x}_{mj}$, which are collected together in a matrix $\textbf{X}_m$. Each consumer chooses between alternatives according to a multinomial logit model. We denote by $y_{mj}$ the number of decision-makers who choose option $j$, and we collect these outcomes in a vector $\textbf{Y}_m$. Mathematically:
\begin{equation}
\begin{aligned}
	& \textbf{Y}_m && \sim Multinomial(N_m, \textbf{p}_m) \\
	& p_{mj} && = \int_{\omega_r} \tilde{s}_{j}(\textbf{X}_m; \omega_r) f(\omega_r) d \omega_r   \hspace*{5pt} \text{for all } j \\
	& \tilde{s}_{j}(\textbf{X}_m; \omega_r)  && =  \exp[V(\textbf{x}_{mj}, \omega_r)] \Big/ \sum_k \exp[V(\textbf{x}_{mk}, \omega_r)] \hspace*{5pt} \text{for all } j
\end{aligned}
\end{equation}
where $p_{mj}$ denotes the unconditional probability that a decision-maker (drawn at random from distribution $f$) will choose alternative $j$, and $V(\textbf{x}_{mj}, \omega_r)$ denotes the utility derived by the decision-maker as a function of the characteristics of the alternative chosen and the tastes of the decision-maker.


\section{Case of a linear utility with unobserved shocks $\xi$}
In that case:
\begin{equation}
\begin{aligned}
	& V(\textbf{x}_{mj}, \omega_r) && = \textbf{x}_{mj} \cdot \omega_r + \xi_{mj} \hspace*{20pt} \text{for all } j \\
\end{aligned}
\end{equation}

\subsubsection{Case of large $N_m$}
If $N_m$ is (sufficiently) large, the observed shares are approximately equal to the theoretical shares:
\begin{equation}
	\hat{p}_{mj} \approx p_{mj} \hspace*{30pt} \text{where } \hat{p}_{mj} = \frac{y_{mj}}{N_m}
\end{equation}
Under this approximation, and assuming moment conditions on the shocks $\xi$, the parameters of the model can be estimated by GMM. There are different approaches, including a nested fixed point algorithm (Berry 1994), and the MPEC approach (DubÃ© et al. 2012).



\section{Case of a linear utility without unobserved shocks}
In that case:
\begin{equation}
\begin{aligned}
	& V(\textbf{x}_{mj}, \omega_r) && = \textbf{x}_{mj} \cdot \omega_r \hspace*{20pt} \text{for all } j \\
\end{aligned}
\end{equation}
We can write down the likelihood function and estimate the model by maximum likelihood.


\paragraph{Log-likelihood and derivatives: \\}
\begin{equation}
\begin{aligned}
	& LL && = \sum_m \sum_{j} y_{mj} \log(p_{mj}) + C \hspace*{15pt}  \text{where } \hspace*{5pt} C =\sum_m \left[ \log(N_m!) - \sum_{j=0}^J \log(y_{mj}!) \right] \\
	& \frac{\partial LL}{\partial \alpha} && = \sum_m \sum_{j} y_{mj} \frac{\partial \log(p_{mj})}{\partial \alpha} \\
	& \frac{\partial^2 LL}{\partial \alpha \partial \alpha'} && = \sum_m \sum_{j} y_{mj} \frac{\partial^2 \log(p_{mj})}{\partial \alpha \partial \alpha'} \\
\end{aligned}
\end{equation}

\subsection{Case of discrete distribution of heterogeneity (latent class)}
There are $R$ classes, indexed by $r$. Each class has size $\pi_r$ such that $\sum_r \pi_r = 1$. \\
Some of the elements of $\omega_r$ may be homogenous and others are heterogenous. We can write:
\begin{equation}
\begin{aligned}
	& p_{mj} && = \sum_r \pi_r \frac{e^{V_{mjr}}}{\sum_k e^{V_{mkr}}} \\
	& V_{mjr} && = \textbf{x}_{mj} \cdot \omega_r \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta + \textbf{x}^{(2)}_{mj} \cdot \gamma_r  \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta + \left[ \textbf{x}^{(2)}_{mj} \times \mathbbm{1}\{r = 1\} , ... ,  \textbf{x}^{(2)}_{mj} \times \mathbbm{1}\{r = R\} \right] \cdot \left[ \gamma_1; ... \gamma_R \right]  \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta  + \tilde{\textbf{x}}^{(2)}_{mjr} \cdot \tilde{\gamma}  \\
\text{where } \hspace*{5pt} & \tilde{\textbf{x}}^{(2)}_{mjr} && = \left[ \textbf{x}^{(2)}_{mj} \times \mathbbm{1}\{r = 1\} , ... ,  \textbf{x}^{(2)}_{mj} \times \mathbbm{1}\{r = R\} \right] \\
\text{and } \hspace*{5pt} & \tilde{\gamma} && = \left[ \gamma_1; ... \gamma_R \right] \\
\end{aligned}
\end{equation}
\underline{Note 1:} For identification, it may be necessary to set some of the coefficients $\gamma_r$ to zero. \\
\underline{Note 2:} To prevent label-switching problems, it is a good idea to sort latent classes by decreasing size $\pi_r$. \\


\subsection{Case of multivariate Normal distribution of heterogeneity}
Some of the elements of $\omega_r$ may be homogenous and others are heterogenous. We can write:
\begin{equation}
\begin{aligned}
	& V(\textbf{x}_{mj}, \omega_r) && = \textbf{x}_{mj} \cdot \omega_r \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta + \textbf{x}^{(2)}_{mj} \cdot \gamma_r && \text{where } \gamma_r \sim MVN(\bar{\gamma}, \Sigma)  \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta + \textbf{x}^{(2)}_{mj} \cdot (\bar{\gamma} + \eta_r) && \text{where } \eta_r \sim MVN(0, \Sigma)  \\
	& && = \textbf{x}^{(1)}_{mj} \cdot \beta + \textbf{x}^{(2)}_{mj} \cdot \bar{\gamma} + \textbf{x}^{(2)}_{mj} \cdot \eta_r && \text{where } \eta_r \sim MVN(0, \Sigma) 
\end{aligned}
\end{equation}
If we assume $\Sigma$ to be diagonal with diagonal elements denoted by $\sigma^2_1, ..., \sigma^2_L$ (where $L$ is the number of heterogenous charaxteristics $\textbf{x}^{(2)}_{mj}$), then: 
\begin{equation}
\begin{aligned}
	& \eta_r && = \left[ \sigma_1 \nu_{r1},  \sigma_2 \nu_{r2}, ...,  \sigma_L \nu_{rL} \right]' \\
	\text{where } & \nu_r && = \left[ \nu_{r1},  \nu_{r2}, ...,  \nu_{rL} \right]'  \sim MVN(0, I)
\end{aligned}
\end{equation}
\underline{Note:} One constraint is that we must have $\sigma_l > 0$ for all $l$. Since the integral will be evaluated numerically by taking a number of ``draws" of $\nu_r$, the same likelihood should be obtained by $\sigma_l$ and $(-\sigma_l)$ if these draws are symmetric.
\\
\\
Together:
\begin{equation}
\begin{aligned}
	& V(\textbf{x}_{mj}, \nu_r) && = \textbf{x}^{(1)}_{mj} \cdot \beta + \textbf{x}^{(2)}_{mj} \cdot \bar{\gamma} + \sum_{l=1}^L \sigma_l \textbf{x}^{(2)}_{mjl} \cdot \nu_{rl} \\
	&  && = \boxed{ \tilde{\textbf{x}}^{(1)}_{mj} \cdot \tilde{\beta} + \tilde{\textbf{x}}^{(2)}_{mjr} \cdot \sigma } \\
	\text{where } \hspace*{5pt} & \tilde{\textbf{x}}^{(1)}_{mj} && = \left[ \textbf{x}^{(1)}_{mj},  \textbf{x}^{(2)}_{mj} \right] = \textbf{x}_{mj} \\
	\text{ } & \tilde{\beta} && = \left[\beta,  \bar{\gamma} \right]' \\
	& \tilde{\textbf{x}}^{(2)}_{mjr} && = \left[ \textbf{x}^{(2)}_{mj1}  \nu_{r1} , ..., \textbf{x}^{(2)}_{mjL}  \nu_{rL}  \right] \\
	\text{ } & \sigma && = \left[\sigma_1, ..., \sigma_L \right]' \\
\end{aligned}
\end{equation}

\paragraph{Approximate integration: \\}
To evaluate the unconditional probabilities $p_{mj}$, we take draws of $\nu_r$ with corresponding weights $w_r$ (using either quadrature or Monte Carlo integration):
\begin{equation}
\begin{aligned}
	& p_{mj} && \approx \sum_r w_r  \tilde{p}_{mjr} \\
	\text{where} \hspace*{5pt} & \tilde{p}_{mjr} && =  \frac{\exp[V_{mjr}]}{ \sum_k \exp[V_{mkr}]} \\
	\text{and} \hspace*{5pt} & V_{mjr} && = V(\textbf{x}_{mj}, \nu_r) = \tilde{\textbf{x}}^{(1)}_{mj} \cdot \tilde{\beta} + \tilde{\textbf{x}}^{(2)}_{mjr} \cdot \sigma \\
\end{aligned}
\end{equation}

\paragraph{Useful derivatives: \\}
\begin{equation}
\small
\begin{aligned}
	& \frac{\partial \tilde{p}_{mjr}}{\partial \tilde{\beta}_a} && =  \tilde{p}_{mjr} \left[ X_{mja}^{(1)} - \sum_k \tilde{p}_{mkr} X_{mka}^{(1)}  \right] \\
	& \frac{\partial \tilde{p}_{mjr}}{\partial \sigma_l} && =  \tilde{p}_{mjr} \left[ X_{mjrl}^{(2)} - \sum_k \tilde{p}_{mkr} X_{mkrl}^{(2)}  \right] \\
	& \frac{\partial^2 \tilde{p}_{mjr}}{\partial \tilde{\beta}_a \partial \tilde{\beta}_{a'}} && = \tilde{p}_{mjr} \left[ \left( X_{mja}^{(1)} - \sum_{k} \tilde{p}_{mkr} X^{(1)}_{mka} \right) \left( X_{mja'}^{(1)} - \sum_{k} \tilde{p}_{mkr} X^{(1)}_{mka'} \right) \right.  \\
	& && \left. + \left( \sum_k \tilde{p}_{mkr} X^{(1)}_{mka} \right) \left( \sum_k \tilde{p}_{mkr} X^{(1)}_{mka'} \right) - \sum_k \tilde{p}_{mkr} X_{mka}^{(1)} X_{mka'}^{(1)} \right] \\
	& \frac{\partial^2 \tilde{p}_{mjr}}{\partial \tilde{\beta}_a \partial \sigma_l} && = \tilde{p}_{mjr} \left[ \left( X_{mja}^{(1)} - \sum_{k} \tilde{p}_{mkr} X^{(1)}_{mka} \right) \left( X_{mjrl}^{(2)} - \sum_{k} \tilde{p}_{mkr} X^{(2)}_{mkrl} \right) \right.  \\
	& && \left. + \left( \sum_k \tilde{p}_{mkr} X^{(1)}_{mka} \right) \left( \sum_k \tilde{p}_{mkr} X^{(2)}_{mkrl} \right) - \sum_k \tilde{p}_{mkr} X_{mka}^{(1)} X_{mkrl}^{(2)} \right] \\
	& \frac{\partial^2 \tilde{p}_{mjr}}{\partial \sigma_l \partial \sigma_{l'}} && = \tilde{p}_{mjr} \left[ \left( X_{mjrl}^{(2)} - \sum_{k} \tilde{p}_{mkr} X^{(2)}_{mkrl} \right) \left( X_{mjrl'}^{(2)} - \sum_{k} \tilde{p}_{mkr} X^{(2)}_{mkrl'} \right) \right.  \\
	& && \left. + \left( \sum_k \tilde{p}_{mkr} X^{(2)}_{mkrl} \right) \left( \sum_k \tilde{p}_{mkr} X^{(2)}_{mkrl'} \right) - \sum_k \tilde{p}_{mkr} X_{mkrl}^{(2)} X_{mkrl'}^{(2)} \right] \\
	& \frac{\partial p_{mj}}{\partial \tilde{\beta}_a} && =  p_{mj} X_{mja}^{(1)} - \sum_k X_{mka}^{(1)} \sum_r w_r \tilde{p}_{mjr} \tilde{p}_{mkr}  =  p_{mj} X_{mja}^{(1)} - \sum_r w_r \tilde{p}_{mjr} \sum_k \tilde{p}_{mkr} X_{mka}^{(1)} \\
	& \frac{\partial p_{mj}}{\partial \sigma_l} && = \sum_r w_r \tilde{p}_{mjr} \left[ X_{mjrl}^{(2)} - \sum_k \tilde{p}_{mkr} X_{mkrl}^{(2)} \right] \\
	& \frac{\partial^2 p_{mj}}{\partial \tilde{\beta}_a \partial \tilde{\beta}_{a'}} && = \sum_r w_r  \frac{\partial^2 \tilde{p}_{mjr}}{\partial \tilde{\beta}_a \partial \tilde{\beta}_{a'}}  \\
	& \frac{\partial^2 p_{mj}}{\partial \tilde{\beta}_a \partial \sigma_l} && = \sum_r w_r  \frac{\partial^2 \tilde{p}_{mjr}}{\partial \tilde{\beta}_a \partial \sigma_l}  \\
	& \frac{\partial^2 p_{mj}}{\partial \sigma_l \partial \sigma_{l'}} && = \sum_r w_r  \frac{\partial^2 \tilde{p}_{mjr}}{\partial \sigma_l \partial \sigma_{l'}} \\
	& \frac{\partial \log(p_{mj})}{\partial \tilde{\beta}_a} && = X_{mja}^{(1)} - \sum_k X_{mka}^{(1)} \sum_r w_r \frac{\tilde{p}_{mjr} \tilde{p}_{mkr}}{p_{mj}} = X_{mja}^{(1)} - \sum_r w_r \frac{\tilde{p}_{mjr} }{p_{mj}} \sum_k \tilde{p}_{mkr} X_{mka}^{(1)}  \\
	& \frac{\partial \log(p_{mj})}{\partial \sigma_l} && = \frac{1}{p_{mj}} \sum_r w_r \tilde{p}_{mjr} \left[ X_{mjrl}^{(2)} - \sum_k \tilde{p}_{mkr} X_{mkrl}^{(2)} \right] \\
	& \frac{\partial^2 \log(p_{mj})}{\partial \tilde{\beta}_a \partial \tilde{\beta}_{a'}} && = - \frac{1}{p_{mj}^2} \times \frac{\partial p_{mj}}{\partial \beta_a} \times \frac{\partial p_{mj}}{\partial \beta_a'} + \frac{1}{p_{mj}} \times \frac{\partial^2 p_{mj}}{\partial \beta_a \partial \beta_{a'}}  \\
	& \frac{\partial^2 \log(p_{mj})}{\partial \tilde{\beta}_a \partial \sigma_l} && = - \frac{1}{p_{mj}^2} \times \frac{\partial p_{mj}}{\partial \beta_a} \times \frac{\partial p_{mj}}{\partial \sigma_l} + \frac{1}{p_{mj}} \times \frac{\partial^2 p_{mj}}{\partial \beta_a \partial \sigma_l}  \\
	& \frac{\partial^2 \log(p_{mj})}{\partial \sigma_l \partial \sigma_{l'}} && = - \frac{1}{p_{mj}^2} \times \frac{\partial p_{mj}}{\partial \sigma_l} \times \frac{\partial p_{mj}}{\partial \sigma_{l'}} + \frac{1}{p_{mj}} \times \frac{\partial^2 p_{mj}}{\partial \sigma_l \partial \sigma_{l'}}  \\
\end{aligned}
\end{equation}

\paragraph{Useful notations: \\}
\begin{equation}
\small
\begin{aligned}
	& A^{(1)}_{mrl} && = \sum_k \tilde{p}_{mkr} X^{(1)}_{mkl} \\
	& B^{(1)}_{mjrl} && = X^{(1)}_{mjl} - A^{(1)}_{mrl}  \\
	& A^{(2)}_{mrl} && = \sum_k \tilde{p}_{mkr} X^{(2)}_{mkrl} \\
	& B^{(2)}_{mjrl} && = X^{(2)}_{mjrl} - A^{(2)}_{mrl}  \\
\end{aligned}
\end{equation}


\paragraph{Log-likelihood and derivatives: \\}
\begin{equation}
\small
\begin{aligned}
	& LL && = \sum_m \sum_{j} y_{mj} \log(p_{mj}) + C \hspace*{15pt}  \text{where } \hspace*{5pt} C =\sum_m \left[ \log(N_m!) - \sum_{j=0}^J \log(y_{mj}!) \right] \\
	& \frac{\partial LL}{\partial \tilde{\beta}_a} && = \sum_m \sum_{j} y_{mj} \frac{\partial \log(p_{mj})}{\partial \tilde{\beta}_a} = \sum_m \sum_{j} y_{mj} \left[ X_{mja}^{(1)} - \frac{1}{p_{mj}} \sum_k X_{mka}^{(1)} \sum_r w_r \tilde{p}_{mjr} \tilde{p}_{mkr} \right] \\
	& \frac{\partial LL}{\partial \sigma_l} && = \sum_m \sum_{j} y_{mj} \frac{\partial \log(p_{mj})}{\partial \sigma_l} = \sum_m \sum_{j} \frac{y_{mj}}{p_{mj}} \sum_r w_r \tilde{p}_{mjr} \left[ X_{mjrl}^{(2)} - \sum_k \tilde{p}_{mkr} X_{mkrl}^{(2)} \right] \\
	& \frac{\partial^2 LL}{\partial \tilde{\beta}_a \partial \tilde{\beta}_{a'}} && = \sum_m \sum_{j} y_{mj} \frac{\partial^2 \log(p_{mj})}{\partial \beta_a \partial \beta_{a'}} \\
	& && = \sum_m \sum_j \frac{y_{mj}}{p_{mj}} \left[ \frac{-1}{p_{mj}} \left( \sum_r w_r \tilde{p}_{mjr} B^{(1)}_{mjra} \right) \left( \sum_r w_r \tilde{p}_{mjr} B^{(1)}_{mjra'} \right) \right. \\
	& && \left. + \sum_r w_r \tilde{p}_{mjr} \left( B^{(1)}_{mjra} B^{(1)}_{mjra'} + A_{mra} A_{mra'} - \sum_k \tilde{p}_{mkr} X^{(1)}_{mkra} X^{(1)}_{mkra'} \right) \right] \\
	& \frac{\partial^2 LL}{\partial \tilde{\beta}_a \partial \sigma_l} && = \sum_m \sum_{j} y_{mj} \frac{\partial^2 \log(p_{mj})}{\partial \tilde{\beta}_a \partial \sigma_l} = \sum_m \sum_{j} y_{mj} \frac{\partial^2 \log(p_{mj})}{\partial \beta_a \partial \sigma_{l}} \\
	& && = \sum_m \sum_j \frac{y_{mj}}{p_{mj}} \left[ \frac{-1}{p_{mj}} \left( \sum_r w_r \tilde{p}_{mjr} B^{(1)}_{mjra} \right) \left( \sum_r w_r \tilde{p}_{mjr} B^{(2)}_{mjrl} \right) \right. \\
	& && \left. + \sum_r w_r \tilde{p}_{mjr} \left( B^{(1)}_{mjra} B^{(2)}_{mjrl} + A^{(1)}_{mra} A^{(2)}_{mrl} - \sum_k \tilde{p}_{mkr} X^{(1)}_{mka} X^{(2)}_{mkrl} \right) \right] \\
	& \frac{\partial^2 LL}{\partial \sigma_l \partial \sigma_{l'}} && = \sum_m \sum_{j} y_{mj} \frac{\partial^2 \log(p_{mj})}{\partial \sigma_l \partial \sigma_{l'}} \\
	& && = \sum_m \sum_j \frac{y_{mj}}{p_{mj}} \left[ \frac{-1}{p_{mj}} \left( \sum_r w_r \tilde{p}_{mjr} B^{(2)}_{mjrl} \right) \left( \sum_r w_r \tilde{p}_{mjr} B^{(2)}_{mjrl'} \right) \right. \\
	& && \left. + \sum_r w_r \tilde{p}_{mjr} \left( B^{(2)}_{mjrl} B^{(2)}_{mjrl'} + A_{mrl} A_{mrl'} - \sum_k \tilde{p}_{mkr} X^{(2)}_{mkrl} X^{(2)}_{mkrl'} \right) \right] \\
\end{aligned}
\end{equation}


\paragraph{Fisher information matrix: \\}
In expectation, the second-degree derivatives simpify:
\begin{equation}
\begin{aligned}
	& \mathbb{E}\left[ \frac{\partial^2 LL}{\partial \beta_a \partial \beta_{a'}} \right] && = - \sum_m \sum_j N_m p_{mj} M^{(1)}_{mja} M^{(1)}_{mja'} \\
	& \mathbb{E}\left[ \frac{\partial^2 LL}{\partial \beta_a \partial \sigma_{l}} \right] && = - \sum_m \sum_j N_m p_{mj} M^{(1)}_{mja} M^{(2)}_{mjl} \\
	& \mathbb{E}\left[ \frac{\partial^2 LL}{\partial \sigma_l \partial \sigma_{l'}} \right] && = - \sum_m \sum_j N_m p_{mj} M^{(2)}_{mjl} M^{(2)}_{mjl'} \\
\text{where }  & M^{(1)}_{mja} && = \sum_r \frac{w_r \tilde{p}_{mjr}}{p_{mj}} B^{(1)}_{mjra} = \frac{\partial \log(p_{mj})}{\partial \beta_a}   \\
\text{where }  & M^{(2)}_{mjl} && = \sum_r \frac{w_r \tilde{p}_{mjr}}{p_{mj}} B^{(2)}_{mjrl} = \frac{\partial \log(p_{mj})}{\partial \sigma_l}   \\
\end{aligned}
\end{equation}


\end{document}

